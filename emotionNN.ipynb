{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Activation, Conv1D, GlobalMaxPooling1D\n",
    "from keras import regularizers, initializers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that maps words to ids. More frequent words have lower ids.\n",
    "    The dictionary contains at the vocab_size-1 most frequent words (and a placeholder '<unk>' for unknown words).\n",
    "    The place holder has the id 0.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for tokens in texts:\n",
    "        counter.update(tokens)\n",
    "    vocab = [w for w, c in counter.most_common(vocab_size - 1)]\n",
    "    word_to_id = {w: (i + 1) for i, w in enumerate(vocab)}\n",
    "    word_to_id[UNKNOWN_TOKEN] = 0\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def to_ids(words, dictionary):\n",
    "    \"\"\"\n",
    "    Takes a list of words and converts them to ids using the word2id dictionary.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        ids.append(dictionary.get(word, dictionary[UNKNOWN_TOKEN]))\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_data(train_file, dev_file):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    trainDF = pd.read_csv(train_file, sep='\\t')\n",
    "    devDF = pd.read_csv(dev_file, sep='\\t')\n",
    "\n",
    "    allDF = pd.concat([trainDF, devDF], ignore_index=True)\n",
    "    allDF = allDF.reindex(np.random.permutation(allDF.index))\n",
    "    allDF.insert(1, 'tweet_tokenized', (allDF['Tweet'].apply(lambda x: tokenizer.tokenize(x))))\n",
    "\n",
    "    word2id = create_dictionary(allDF[\"tweet_tokenized\"], VOCAB_SIZE)\n",
    "\n",
    "    allDF.insert(1, 'tweet_ids', (allDF['Tweet'].apply(lambda x: to_ids(x, dictionary=word2id))))\n",
    "\n",
    "    allDF['all'] = allDF.iloc[:, -11:].values.tolist()\n",
    "\n",
    "    # calc class weights\n",
    "    # class_weights = compute_class_weights(allDF)\n",
    "    class_weights = None    \n",
    "    # print(class_weights)\n",
    "\n",
    "    total = len(allDF)\n",
    "    trainend = int(total * 0.8)\n",
    "    devend = trainend + int(total * 0.1)\n",
    "    return allDF.iloc[:trainend, :], allDF.iloc[trainend:devend, :], allDF.iloc[devend:, :], class_weights\n",
    "\n",
    "\n",
    "def evaluate(predictions, y_test):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    all_correct = 0\n",
    "    for i, pred in enumerate(predictions):\n",
    "        for j, em in enumerate(pred):\n",
    "            if em >= 0.5:\n",
    "                if y_test[i][j] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            if em < 0.5:\n",
    "                if y_test[i][j] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "            if tp + tn == y_test.shape[1]:\n",
    "                all_correct += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"F1: {}\\nPrecision: {}\\nRecall: {}\\nCompletely correct: {}\".format(f1, precision, recall, all_correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/3_Programming/1_Studium/Python/SemEval2018_Task1_5/data/'\n",
    "train_file = os.path.join(data_dir, '2018-E-c-En-train.txt')\n",
    "dev_file = os.path.join(data_dir, '2018-E-c-En-dev.txt')\n",
    "\n",
    "VOCAB_SIZE = 100000\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 50\n",
    "EPOCHS = 10  # Standard 10\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "EMOTIONS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love',\n",
    "            'optimism', 'pessimism', 'sadness', 'surprise', 'trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, devDF, testDF, class_weights = read_data(train_file, dev_file)\n",
    "x_train = sequence.pad_sequences(np.array(trainDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "x_dev = sequence.pad_sequences(np.array(devDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(np.array(testDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "y_train = np.array([trainDF['all']])[0]\n",
    "y_dev = np.array([devDF['all']])[0]\n",
    "y_test = np.array([testDF['all']])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "cnn_model.add(Conv1D(2 * HIDDEN_SIZE,\n",
    "                     kernel_size=3,\n",
    "                     activation='tanh',\n",
    "                     strides=1,\n",
    "                     padding='valid',\n",
    "                     ))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(11, activation='sigmoid')) #11 = no of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, adam\n",
    "\n",
    "opt = adam(lr=0.01)\n",
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'],\n",
    "                  sample_weight_mode='temporal'\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_acc', patience=5, mode='max')\n",
    "checkpoint = ModelCheckpoint(data_dir+'model.m', save_best_only=True, monitor='val_acc', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18252407795206593,\n 0.5302046507916747,\n 0.1759452443364328,\n 0.46780844029116636,\n 0.17351598806386132,\n 0.6274344503564779,\n 0.2562887503102258,\n 0.6074081062585732,\n 0.2682319716618163,\n 0.7973375187498287,\n 0.8035216677691579]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    class_weights = np.sum(y_train, axis=0)\n",
    "    sum_class_weights = np.sum(y_train)\n",
    "    \n",
    "    class_weights = [((sum_class_weights - i) / sum_class_weights)**10 for i in class_weights]\n",
    "    \n",
    "    return class_weights\n",
    "compute_class_weights(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2 ..., 1 1 1]\n [2 1 2 ..., 1 1 1]\n [2 1 2 ..., 1 1 1]\n ..., \n [1 1 2 ..., 2 1 1]\n [2 1 2 ..., 1 1 1]\n [2 1 1 ..., 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def compute_sample_weights(y_train, factor):\n",
    "    class_weights = np.array(y_train)\n",
    "    class_weights[class_weights == 1] = factor\n",
    "    class_weights[class_weights == 0] = 1\n",
    "    # return np.reshape(class_weights, class_weights.shape + (1,))\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "sample_weights = compute_sample_weights(y_train, 2)\n",
    "print(sample_weights)\n",
    "# print(x_train)\n",
    "# print(y_train)\n",
    "# print(sample_weights)\n",
    "# y_train = np.reshape(y_train, y_train.shape + (1,))\n",
    "# y_dev = np.reshape(y_dev, y_dev.shape + (1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# print(np.expand_dims(y_train,0))\n",
    "# y_train = np.reshape(y_train,[1,y_train.shape[0],y_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found a sample_weight array for an input with shape (6179, 11). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f1d6fd7c2a5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# class_weight=class_weights,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1581\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1582\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1423\u001b[0m         sample_weights = [_standardize_weights(ref, sw, cw, mode)\n\u001b[0;32m   1424\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1425\u001b[1;33m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0m\u001b[0;32m   1426\u001b[0m         \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m         _check_loss_and_target_compatibility(y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                                    self._feed_output_names)\n\u001b[0;32m   1423\u001b[0m         sample_weights = [_standardize_weights(ref, sw, cw, mode)\n\u001b[1;32m-> 1424\u001b[1;33m                           \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[0;32m   1426\u001b[0m         \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[1;34m(y, sample_weight, class_weight, sample_weight_mode)\u001b[0m\n\u001b[0;32m    509\u001b[0m             raise ValueError('Found a sample_weight array for '\n\u001b[0;32m    510\u001b[0m                              \u001b[1;34m'an input with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m                              \u001b[1;34m'Timestep-wise sample weighting (use of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                              \u001b[1;34m'sample_weight_mode=\"temporal\") is restricted to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found a sample_weight array for an input with shape (6179, 11). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    # batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopper, checkpoint],\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_dev, y_dev),\n",
    "    class_weight=class_weights,\n",
    "    # sample_weight=[sample_weights],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "best_model = load_model(data_dir+'model.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 1 0 0]\n[ 0.10966518  0.06967428  0.13928385  0.05048795  0.56468403  0.29400647\n  0.27725214  0.2429215   0.75032073  0.02757213  0.03927426]\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(x_test)\n",
    "print(y_test[0])\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.49065743944636675\nPrecision: 0.6589219330855018\nRecall: 0.3908489525909592\nCompletely correct: 2\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-d7075f8dcc52>, line 4)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-d7075f8dcc52>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    Completely correct: 2\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "F1: 0.4036478984932593\n",
    "Precision: 0.6559278350515464\n",
    "Recall: 0.29152348224513175\n",
    "Completely correct: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unweighted \n",
    "\tF1: 0.27632915678970543\n",
    "\tPrecision: 0.3633125556544969\n",
    "\tRecall: 0.22295081967213115\n",
    "\tCompletely correct: 2"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "F1: 0.10682492581602375 \n",
    "\n",
    "Precision: 0.675 \n",
    "\n",
    "Recall: 0.05800214822771214 \n",
    "\n",
    "Completely correct: 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax\n",
    "F1: 0.13215859030837004\n",
    "Precision: 0.5844155844155844\n",
    "Recall: 0.07450331125827815\n",
    "Completely correct: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Todo:Functional API class:weights 0,1 11 outputs\n",
    "\"It depends on your application. Class weights are useful when training on highly skewed data sets; for example, a classifier to detect fraudulent transactions. Sample weights are useful when you don't have equal confidence in the samples in your batch. A common example is performing regression on measurements with variable uncertainty.\" https://stackoverflow.com/questions/43459317/keras-class-weight-vs-sample-weights-in-the-fit-generator\n",
    "\n",
    "            class_weight: Optional dictionary mapping class indices (integers)\n",
    "                to a weight (float) value, used for weighting the loss function\n",
    "                (during training only).\n",
    "                This can be useful to tell the model to\n",
    "                \"pay more attention\" to samples from\n",
    "                an under-represented class.\n",
    "            sample_weight: Optional Numpy array of weights for\n",
    "                the training samples, used for weighting the loss function\n",
    "                (during training only). You can either pass a flat (1D)\n",
    "                Numpy array with the same length as the input samples\n",
    "                (1:1 mapping between weights and samples),\n",
    "                or in the case of temporal data,\n",
    "                you can pass a 2D array with shape\n",
    "                `(samples, sequence_length)`,\n",
    "                to apply a different weight to every timestep of every sample.\n",
    "                In this case you should make sure to specify\n",
    "                `sample_weight_mode=\"temporal\"` in `compile()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sample weights mode=\"temporal\"\n",
    "matrix mitgewicht für jede klasse in jedem sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
