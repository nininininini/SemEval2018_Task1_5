{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Dense, Embedding, LSTM, Bidirectional, Dropout, Activation,  Conv1D, GlobalMaxPooling1D\n",
    "from keras import regularizers, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that maps words to ids. More frequent words have lower ids.\n",
    "    The dictionary contains at the vocab_size-1 most frequent words (and a placeholder '<unk>' for unknown words).\n",
    "    The place holder has the id 0.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for tokens in texts:\n",
    "        counter.update(tokens)\n",
    "    vocab = [w for w, c in counter.most_common(vocab_size - 1)]\n",
    "    word_to_id = {w: (i + 1) for i, w in enumerate(vocab)}\n",
    "    word_to_id[UNKNOWN_TOKEN] = 0\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def to_ids(words, dictionary):\n",
    "    \"\"\"\n",
    "    Takes a list of words and converts them to ids using the word2id dictionary.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        ids.append(dictionary.get(word, dictionary[UNKNOWN_TOKEN]))\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_data(train_file, dev_file):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    trainDF = pd.read_csv(train_file, sep='\\t')\n",
    "    devDF = pd.read_csv(dev_file, sep='\\t')\n",
    "\n",
    "    allDF = pd.concat([trainDF, devDF], ignore_index=True)\n",
    "    allDF = allDF.reindex(np.random.permutation(allDF.index))\n",
    "    allDF.insert(1, 'tweet_tokenized', (allDF['Tweet'].apply(lambda x: tokenizer.tokenize(x))))\n",
    "\n",
    "    word2id = create_dictionary(allDF[\"tweet_tokenized\"], VOCAB_SIZE)\n",
    "\n",
    "    allDF.insert(1, 'tweet_ids', (allDF['Tweet'].apply(lambda x: to_ids(x, dictionary=word2id))))\n",
    "\n",
    "    allDF['all'] = allDF.iloc[:, -11:].values.tolist()\n",
    "    total = len(allDF)\n",
    "    trainend = int(total * 0.8)\n",
    "    devend = trainend + int(total * 0.1)\n",
    "    return allDF.iloc[:trainend, :], allDF.iloc[trainend:devend, :], allDF.iloc[devend:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/3_Programming/1_Studium/Python/SemEval2018_Task1_5/data/'\n",
    "train_file = os.path.join(data_dir, '2018-E-c-En-train.txt')\n",
    "dev_file = os.path.join(data_dir, '2018-E-c-En-dev.txt')\n",
    "\n",
    "VOCAB_SIZE = 100000\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 20\n",
    "HIDDEN_SIZE = 100\n",
    "EPOCHS = 10  # Standard 10\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "EMOTIONS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love',\n",
    "            'optimism', 'pessimism', 'sadness', 'surprise', 'trust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, devDF, testDF = read_data(train_file, dev_file)\n",
    "x_train = sequence.pad_sequences(np.array(trainDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "x_dev = sequence.pad_sequences(np.array(devDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(np.array(testDF['tweet_ids']), maxlen=MAX_LEN)\n",
    "y_train = np.array([trainDF['all']])[0]\n",
    "y_dev = np.array([devDF['all']])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "cnn_model.add(Conv1D(2 * HIDDEN_SIZE,\n",
    "                     kernel_size=3,\n",
    "                     activation='tanh',\n",
    "                     strides=1,\n",
    "                     padding='valid',\n",
    "                     kernel_regularizer=regularizers.l1(0.001),))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "cnn_model.add(Dense(y_train.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import rmsprop\n",
    "opt=rmsprop(lr=0.0001, decay=1e-6)\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=opt,\n",
    "                   metrics=['accuracy'])\n",
    "#opt war adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6179 samples, validate on 772 samples\nEpoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 6s - loss: 6.0573 - acc: 0.2763 - val_loss: 5.9521 - val_acc: 0.2591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.7743 - acc: 0.2275 - val_loss: 5.6991 - val_acc: 0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.5889 - acc: 0.1622 - val_loss: 5.5839 - val_acc: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.5000 - acc: 0.0918 - val_loss: 5.5144 - val_acc: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.4364 - acc: 0.1055 - val_loss: 5.4566 - val_acc: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.3821 - acc: 0.1172 - val_loss: 5.4079 - val_acc: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.3374 - acc: 0.1134 - val_loss: 5.3674 - val_acc: 0.0570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.3012 - acc: 0.1168 - val_loss: 5.3356 - val_acc: 0.0596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.2742 - acc: 0.1787 - val_loss: 5.3122 - val_acc: 0.0583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 5.2532 - acc: 0.1316 - val_loss: 5.2961 - val_acc: 0.0829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b0526ee80>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_dev, y_dev),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79008341,  0.30411407,  0.80335492, ...,  0.63104343,\n         0.10794483,  0.11674609],\n       [ 0.78734958,  0.30633661,  0.80123734, ...,  0.63051552,\n         0.10968722,  0.11920068],\n       [ 0.78783017,  0.30551025,  0.80141258, ...,  0.63141537,\n         0.10904864,  0.1185305 ],\n       ..., \n       [ 0.78441739,  0.30887508,  0.79674363, ...,  0.62840843,\n         0.1129552 ,  0.12210647],\n       [ 0.78722936,  0.30642501,  0.80032092, ...,  0.62980491,\n         0.10976435,  0.11918981],\n       [ 0.76941305,  0.31876999,  0.78167897, ...,  0.61964798,\n         0.12614734,  0.13672405]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
